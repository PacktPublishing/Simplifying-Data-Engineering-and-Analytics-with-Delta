{"cells":[{"cell_type":"markdown","source":["https://github.com/delta-io/delta/blob/master/examples/cheat_sheet/delta_lake_cheat_sheet.pdf"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Delta Cheat Sheet","showTitle":true,"inputWidgets":{},"nuid":"94de84b4-3697-4c96-80ce-46a5a478f41b"}}},{"cell_type":"markdown","source":["# Batch"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5d81e7a2-64ba-48aa-ae79-da20b897d3d5"}}},{"cell_type":"markdown","source":["## Read\n* By File path\n  * df = spark.read.format(\"parquet\"|\"csv\"|\"json\"|etc.).load('path to delta table')\n* By Table\n  * df = spark.table('delta table name')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6e8dcdd6-6241-4afb-94ae-2e1537f54846"}}},{"cell_type":"markdown","source":["## Write\n* By File path\n  * df.write.format(\"delta\").mode(\"overwrite\"|\"append\").partitionBy('field').save('path to delta table')\n* By Table\n  * df.write.format(\"delta\").option('mergeSchema', \"true\").saveAsTable('delta table name')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d1088737-1241-4502-9998-7331057488ad"}}},{"cell_type":"markdown","source":["# Streaming"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"75e819d0-88bb-4551-8a27-a812af004a21"}}},{"cell_type":"markdown","source":["## Read\n* By File path\n  * df = spark.readStream.format(\"parquet\"|\"csv\"|\"json\"|etc.).schema(schema).load('path to delta table')\n* By Table\n  * df = spark.readStream.format(\"delta\").table('delta table name')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0224e7bc-7673-4fd6-93a4-05877599c64d"}}},{"cell_type":"markdown","source":["## Write\n\n* By File path\n  * df.writeStream.format(\"delta\").outputMode(\"append\"|\"update\"|\"complete\").option(\"checkpointLocation', 'path to chkpoint').trigger(once=True|processingTime=\"x minute\").start('path to delta table')\n* By Table\n  * df.writeStream.format(\"delta\").outputMode(\"append\"|\"update\"|\"complete\").option(\"checkpointLocation', 'path to chkpoint').trigger(once=True|processingTime=\"x minute\").table('delta table name')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ac3c0287-8fcf-488f-b5ee-bc228d482ac5"}}},{"cell_type":"markdown","source":["# Utility Functions"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ca1b10cc-9bbb-4c93-95a1-392a7c69a2e8"}}},{"cell_type":"code","source":["from pyspark.sql.functions import *\nfrom delta.tables import DeltaTable\n\n# Function to upsert microBatchOutputDF into Delta Lake table using merge\ndef upsertToDelta(microBatchOutputDF, batchId):\n    t = deltaTable.alias(\"t\").merge(microBatchOutputDF.alias(\"s\"), \"s.id = t.id\")\\\n        .whenMatchedUpdateAll()\\\n        .whenNotMatchedInsertAll()\\\n        .execute()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b3946186-7724-4fa3-94e4-5ad0d17b62d6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["# Streaming Types\n* File Based\n  * File lands on disk & is streamed from storage\n* Event Based\n  * More real-time and leverages a sstreaming service ssuch as Kafka, Kinesis, Eventhub"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"40ee73f6-a0bb-472b-9c6e-2f4f1135312e"}}},{"cell_type":"code","source":["import shutil\nshutil.rmtree(\"/tmp/ch-4/\", ignore_errors=True)\ndbutils.fs.rm(\"/tmp/ch-4/\", True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"def3dcc8-0b75-4ce7-9462-1812910cd768"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[33]: True</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[33]: True</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## File Based"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5c947b1f-f42d-49db-863e-491829e441dc"}}},{"cell_type":"code","source":["import random\n# Create a table(key, value) of some data\ndata = spark.range(8)\ndata = data.withColumn(\"value\", data.id + random.randint(0, 5000))\ndata.write.format(\"delta\").save(\"/tmp/ch-4/delta-table\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"59a6ece4-b1a4-4cc0-996b-94a714e20599"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\nSELECT * from delta.`/tmp/ch-4/delta-table` LIMIT 5"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c28db90c-3f12-4f5b-9998-1ddaa6ad81a8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[0,1601],[2,1603],[3,1604],[4,1605],[1,1602]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"id","type":"\"long\"","metadata":"{}"},{"name":"value","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>value</th></tr></thead><tbody><tr><td>0</td><td>1601</td></tr><tr><td>2</td><td>1603</td></tr><tr><td>3</td><td>1604</td></tr><tr><td>4</td><td>1605</td></tr><tr><td>1</td><td>1602</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["streamingDf = spark.readStream.format(\"rate\").load()\n#display(streamingDf)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1d8dafc2-d441-4328-a552-3ca2ce0217f2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Stream writes to the table\nstream = streamingDf.selectExpr(\"value as id\").writeStream\\\n    .format(\"delta\")\\\n    .option(\"checkpointLocation\", \"/tmp/ch-4/checkpoint\")\\\n    .start(\"/tmp/ch-4/delta-table2\")\nstream.awaitTermination(10)\nstream.stop()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"be516b90-9fb1-463c-abe3-2f8ab9a9d519"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\nSELECT * from delta.`/tmp/ch-4/delta-table2` LIMIT 5"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"10365afe-b45f-406a-8b87-8aaa0f3a69e6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[1],[3],[4],[0],[2]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"id","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th></tr></thead><tbody><tr><td>1</td></tr><tr><td>3</td></tr><tr><td>4</td></tr><tr><td>0</td></tr><tr><td>2</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Stream reads from a table\nstream2 = spark.readStream.format(\"delta\").load(\"/tmp/ch-4/delta-table2\")\\\n    .writeStream\\\n    .format(\"console\")\\\n    .start()\nstream2.awaitTermination(10)\nstream2.stop()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bd344a7a-bd1e-40ba-a03f-ab6e475f84dd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### In-Stream Trasformations"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b1cb4728-06b2-4ef4-841c-2a8477c5c11f"}}},{"cell_type":"code","source":["# In-stream transformations\nstreamingAggregatesDF = spark.readStream.format(\"rate\").load()\\\n    .withColumn(\"id\", col(\"value\") % 10)\\\n    .drop(\"timestamp\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bfc81d29-c771-4edd-9906-e07226fba4fa"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Write the output of a streaming aggregation query into Delta Lake table\ndeltaTable = DeltaTable.forPath(spark, \"/tmp/ch-4/delta-table\")\nprint(\"Before\")\ndeltaTable.toDF().show()\n\nstream3 = streamingAggregatesDF.writeStream\\\n    .format(\"delta\") \\\n    .foreachBatch(upsertToDelta) \\\n    .outputMode(\"update\") \\\n    .start()\nstream3.awaitTermination(10)\nstream3.stop()\n\nprint(\"After\")\ndeltaTable.toDF().show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5359e3d9-e716-4c23-8010-fa64eb429c91"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Before\n+---+-----+\n| id|value|\n+---+-----+\n|  0| 1601|\n|  6| 1607|\n|  2| 1603|\n|  5| 1606|\n|  3| 1604|\n|  7| 1608|\n|  4| 1605|\n|  1| 1602|\n+---+-----+\n\nAfter\n+---+-----+\n| id|value|\n+---+-----+\n|  0|    0|\n|  1|    1|\n|  2|    2|\n|  3|    3|\n|  4|    4|\n|  5|    5|\n|  6|    6|\n|  7|    7|\n+---+-----+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Before\n+---+-----+\n id|value|\n+---+-----+\n  0| 1601|\n  6| 1607|\n  2| 1603|\n  5| 1606|\n  3| 1604|\n  7| 1608|\n  4| 1605|\n  1| 1602|\n+---+-----+\n\nAfter\n+---+-----+\n id|value|\n+---+-----+\n  0|    0|\n  1|    1|\n  2|    2|\n  3|    3|\n  4|    4|\n  5|    5|\n  6|    6|\n  7|    7|\n+---+-----+\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Delta table as both a streaming source & sink"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"80b5b5b0-f958-45cd-93cf-3cf9ba655e60"}}},{"cell_type":"code","source":["from_tbl = \"/tmp/ch-4/from_delta\"\nto_tbl = \"/tmp/ch-4/to_delta\"\nnumRows = 10\nspark.range(numRows).write.mode(\"overwrite\").format(\"delta\").save(from_tbl)\nspark.read.format(\"delta\").load(from_tbl).show()\nspark.range(numRows, numRows * 10).write.mode(\"overwrite\").format(\"delta\").save(to_tbl)\n\nstream4 = spark.readStream.format(\"delta\").load(to_tbl).writeStream.format(\"delta\")\\\n    .option(\"checkpointLocation\", \"/tmp/ch-4/checkpoint/tbl1\") \\\n    .outputMode(\"append\") \\\n    .start(from_tbl)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f3634f57-a79e-433d-aff8-1e4c17ee6d07"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+---+\n| id|\n+---+\n|  1|\n|  2|\n|  7|\n|  3|\n|  9|\n|  6|\n|  0|\n|  4|\n|  8|\n|  5|\n+---+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+\n id|\n+---+\n  1|\n  2|\n  7|\n  3|\n  9|\n  6|\n  0|\n  4|\n  8|\n  5|\n+---+\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# repartition table while streaming job is running\nspark.read.format(\"delta\").load(to_tbl).repartition(10).write\\\n    .format(\"delta\")\\\n    .mode(\"overwrite\")\\\n    .option(\"dataChange\", \"false\")\\\n    .save(to_tbl)\n\nstream4.awaitTermination(10)\nstream4.stop()\n#After streaming write \nspark.read.format(\"delta\").load(from_tbl).show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"126806c8-b538-4889-9179-102fd2057d48"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+---+\n| id|\n+---+\n|  1|\n|  2|\n|  3|\n|  0|\n|  4|\n+---+\nonly showing top 5 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+\n id|\n+---+\n  1|\n  2|\n  3|\n  0|\n  4|\n+---+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Event Based"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"58abc65c-7789-4272-bc22-57c7cbd09068"}}},{"cell_type":"markdown","source":["### Kafka\n* Replace host/port & topic name"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1b9cdfa4-18a1-4856-937b-975eb487988e"}}},{"cell_type":"code","source":["# Example of reading from Kafka \n'''\nfrom pyspark.sql.functions import col \nkafkaServer = \"<host:port>\"                        # Specify Host & Port & the name of the topic \ntopicName = 'iot-topic' \n\niotData = (spark.readStream                         # Get the DataStreamReader \n  .format(\"kafka\")                                  # Specify the source format as \"kafka\" \n  .option(\"kafka.bootstrap.servers\", kafkaServer)   # Configure the Kafka server name and port \n  .option(\"subscribe\", topicName)                   # Subscribe to the Kafka topic \n  .option(\"startingOffsets\", \"latest\")              # stream to latest when we restart notebook \n  .option(\"maxOffsetsPerTrigger\", 1000)             # Throttle Kafka's processing of the streams \n  .load() \n  .repartition(8) \n  .select(col(\"value\").cast(\"STRING\")) \n) \n'''"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ccf616c3-0684-4c85-a853-8a7fd1817c46"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[42]: &#39;\\nfrom pyspark.sql.functions import col \\nkafkaServer = &#34;&lt;host:port&gt;&#34;                        # Specify Host &amp; Port &amp; the name of the topic \\ntopicName = \\&#39;iot-topic\\&#39; \\n\\niotData = (spark.readStream                         # Get the DataStreamReader \\n  .format(&#34;kafka&#34;)                                  # Specify the source format as &#34;kafka&#34; \\n  .option(&#34;kafka.bootstrap.servers&#34;, kafkaServer)   # Configure the Kafka server name and port \\n  .option(&#34;subscribe&#34;, topicName)                   # Subscribe to the Kafka topic \\n  .option(&#34;startingOffsets&#34;, &#34;latest&#34;)              # stream to latest when we restart notebook \\n  .option(&#34;maxOffsetsPerTrigger&#34;, 1000)             # Throttle Kafka\\&#39;s processing of the streams \\n  .load() \\n  .repartition(8) \\n  .select(col(&#34;value&#34;).cast(&#34;STRING&#34;)) \\n) \\n&#39;</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[42]: &#39;\\nfrom pyspark.sql.functions import col \\nkafkaServer = &#34;&lt;host:port&gt;&#34;                        # Specify Host &amp; Port &amp; the name of the topic \\ntopicName = \\&#39;iot-topic\\&#39; \\n\\niotData = (spark.readStream                         # Get the DataStreamReader \\n  .format(&#34;kafka&#34;)                                  # Specify the source format as &#34;kafka&#34; \\n  .option(&#34;kafka.bootstrap.servers&#34;, kafkaServer)   # Configure the Kafka server name and port \\n  .option(&#34;subscribe&#34;, topicName)                   # Subscribe to the Kafka topic \\n  .option(&#34;startingOffsets&#34;, &#34;latest&#34;)              # stream to latest when we restart notebook \\n  .option(&#34;maxOffsetsPerTrigger&#34;, 1000)             # Throttle Kafka\\&#39;s processing of the streams \\n  .load() \\n  .repartition(8) \\n  .select(col(&#34;value&#34;).cast(&#34;STRING&#34;)) \\n) \\n&#39;</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Kinesis\n* Replace Kinesis instance details - streamName, region\n* The following example assumes incoming data to have following schema\n  * id, user_id, device_id, num_steps, miles_walked, calories_burnt, timestamp"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e5dd2b83-d6b2-458f-b34d-fd06a932f94f"}}},{"cell_type":"code","source":["# Example of reading from Kinesis to pick up iot device data reporting on a user’s fitness metrics \n'''\nfrom pyspark.sql.functions import * \nkinesisDF = spark.readStream \\ \n  .format(\"kinesis\") \\ \n  .option(\"streamName\", \"kinesis-stream\") \\ \n  .option(\"region\", \"us-east-2\") \\\n  .option(\"initialPosition\", \"trim_horizon\") \\\n  .load() \n\ndataDF = kinesisDF.select(col(\"data\").cast('string').alias(\"data\"))\ndataDF.createOrReplaceTempView(\"stream_data\") \n\nstream_df = spark.sql(\"\"\"\n    SELECT data:id,  \n    data:user_id, \n    data:device_id, \n    cast(data:num_steps as int) as num_steps,  \n    cast(data:miles_walked as double) as miles_walked,  \n    cast(data:calories_burnt as double) as calories_burnt, \n    cast(data:timestamp as timestamp) as timestamp\n    FROM stream_data\"\"\") \n\n(stream_df .writeStream.format(\"delta\") \n   .trigger(processingTime='30 seconds') \n   .option(\"checkpointLocation\", <checkpoint location in storage>)\n   .outputMode(\"append\")\n   .table(\"device_data_streaming\")) \n'''"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7c494a09-692d-4c1a-bea7-497b32e91e76"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[43]: &#39;\\nfrom pyspark.sql.functions import * \\nkinesisDF = spark.readStream \\\\ \\n  .format(&#34;kinesis&#34;) \\\\ \\n  .option(&#34;streamName&#34;, &#34;kinesis-stream&#34;) \\\\ \\n  .option(&#34;region&#34;, &#34;us-east-2&#34;)   .option(&#34;initialPosition&#34;, &#34;trim_horizon&#34;)   .load() \\n\\ndataDF = kinesisDF.select(col(&#34;data&#34;).cast(\\&#39;string\\&#39;).alias(&#34;data&#34;))\\ndataDF.createOrReplaceTempView(&#34;stream_data&#34;) \\n\\nstream_df = spark.sql(&#34;&#34;&#34;\\n    SELECT data:id,  \\n    data:user_id, \\n    data:device_id, \\n    cast(data:num_steps as int) as num_steps,  \\n    cast(data:miles_walked as double) as miles_walked,  \\n    cast(data:calories_burnt as double) as calories_burnt, \\n    cast(data:timestamp as timestamp) as timestamp\\n    FROM stream_data&#34;&#34;&#34;) \\n\\n(stream_df .writeStream.format(&#34;delta&#34;) \\n   .trigger(processingTime=\\&#39;30 seconds\\&#39;) \\n   .option(&#34;checkpointLocation&#34;, &lt;checkpoint location in storage&gt;)\\n   .outputMode(&#34;append&#34;)\\n   .table(&#34;device_data_streaming&#34;)) \\n&#39;</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[43]: &#39;\\nfrom pyspark.sql.functions import * \\nkinesisDF = spark.readStream \\\\ \\n  .format(&#34;kinesis&#34;) \\\\ \\n  .option(&#34;streamName&#34;, &#34;kinesis-stream&#34;) \\\\ \\n  .option(&#34;region&#34;, &#34;us-east-2&#34;)   .option(&#34;initialPosition&#34;, &#34;trim_horizon&#34;)   .load() \\n\\ndataDF = kinesisDF.select(col(&#34;data&#34;).cast(\\&#39;string\\&#39;).alias(&#34;data&#34;))\\ndataDF.createOrReplaceTempView(&#34;stream_data&#34;) \\n\\nstream_df = spark.sql(&#34;&#34;&#34;\\n    SELECT data:id,  \\n    data:user_id, \\n    data:device_id, \\n    cast(data:num_steps as int) as num_steps,  \\n    cast(data:miles_walked as double) as miles_walked,  \\n    cast(data:calories_burnt as double) as calories_burnt, \\n    cast(data:timestamp as timestamp) as timestamp\\n    FROM stream_data&#34;&#34;&#34;) \\n\\n(stream_df .writeStream.format(&#34;delta&#34;) \\n   .trigger(processingTime=\\&#39;30 seconds\\&#39;) \\n   .option(&#34;checkpointLocation&#34;, &lt;checkpoint location in storage&gt;)\\n   .outputMode(&#34;append&#34;)\\n   .table(&#34;device_data_streaming&#34;)) \\n&#39;</div>"]}}],"execution_count":0},{"cell_type":"code","source":["'''\n#hold the lookup table in a dataframe\ndevices_df = spark.table(\"devices_lookup_tbl\") \n\n#read the incoming iot data \niot_df = spark.readStream() …. \n\n#join the 2 dataframes on device identifier \njoin_df = iot_df.join(devices_df, [‘device_id])\n\n#persist to disk\njoin_df.writeStream \\ \n  .format('delta') \\ \n  .outputMode('append') \\ \n  .option('checkpointLocation', checkpoint_path) \\ \n  .toTable(\"”devices_iot_tbl\") \n\n# At this point if new devices get registered, the devices_df will get the updates \n# Pre-delta, this would be stale data and would require the user to re-read the table each time prior to a join\n\nuniqueVisitors =  iot_df\n     .withWatermark(\"event_time\", \"10 minutes\") \n     .dropDuplicates(\"event_time\", \"uid\") \n'''"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"00ac7b74-b7a5-4c89-a87a-c934cfbeb6cd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[44]: &#39;\\n#hold the lookup table in a dataframe\\ndevices_df = spark.table(&#34;devices_lookup_tbl&#34;) \\n\\n#read the incoming iot data \\niot_df = spark.readStream() …. \\n\\n#join the 2 dataframes on device identifier \\njoin_df = iot_df.join(devices_df, [‘device_id])\\n\\n#persist to disk\\njoin_df.writeStream \\\\ \\n  .format(\\&#39;delta\\&#39;) \\\\ \\n  .outputMode(\\&#39;append\\&#39;) \\\\ \\n  .option(\\&#39;checkpointLocation\\&#39;, checkpoint_path) \\\\ \\n  .toTable(&#34;”devices_iot_tbl&#34;) \\n\\n# At this point if new devices get registered, the devices_df will get the updates \\n# Pre-delta, this would be stale data and would require the user to re-read the table each time prior to a join\\n\\nuniqueVisitors =  iot_df\\n     .withWatermark(&#34;event_time&#34;, &#34;10 minutes&#34;) \\n     .dropDuplicates(&#34;event_time&#34;, &#34;uid&#34;) \\n&#39;</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[44]: &#39;\\n#hold the lookup table in a dataframe\\ndevices_df = spark.table(&#34;devices_lookup_tbl&#34;) \\n\\n#read the incoming iot data \\niot_df = spark.readStream() …. \\n\\n#join the 2 dataframes on device identifier \\njoin_df = iot_df.join(devices_df, [‘device_id])\\n\\n#persist to disk\\njoin_df.writeStream \\\\ \\n  .format(\\&#39;delta\\&#39;) \\\\ \\n  .outputMode(\\&#39;append\\&#39;) \\\\ \\n  .option(\\&#39;checkpointLocation\\&#39;, checkpoint_path) \\\\ \\n  .toTable(&#34;”devices_iot_tbl&#34;) \\n\\n# At this point if new devices get registered, the devices_df will get the updates \\n# Pre-delta, this would be stale data and would require the user to re-read the table each time prior to a join\\n\\nuniqueVisitors =  iot_df\\n     .withWatermark(&#34;event_time&#34;, &#34;10 minutes&#34;) \\n     .dropDuplicates(&#34;event_time&#34;, &#34;uid&#34;) \\n&#39;</div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3c125a25-7156-43ee-b0ec-d4d2b970e437"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Ch-04-DeltaForBatch&Streaming","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":386413837244879}},"nbformat":4,"nbformat_minor":0}
