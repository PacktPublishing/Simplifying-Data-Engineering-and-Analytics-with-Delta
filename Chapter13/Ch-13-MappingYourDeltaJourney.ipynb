{"cells":[{"cell_type":"markdown","source":["# Clean prior run data files"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bdec79ed-4c9d-4e65-9167-db5668b7ea9b"}}},{"cell_type":"code","source":["dbutils.fs.rm('/tmp/ch-13/', True)\n\n# Drop & recreate database\nspark.sql(\"DROP DATABASE IF EXISTS ch_13 CASCADE\")\nspark.sql(\"CREATE DATABASE ch_13 \")\nspark.sql(\"USE ch_13\")\n\n# Configure Path\nDELTALAKE_PATH = \"/tmp/ch-13/data\"\n\n# Remove table if it exists\ndbutils.fs.rm(DELTALAKE_PATH, recurse=True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f3c0b859-d57b-46d3-8fdf-116b20670537"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# Migration to Delta Operational Simplification\n* No ned to <b> Refresh Table</b> as Delta always return the most up-to-date information\n* No ned to run Table repair commands such as <b> MSCK </b> or babysit partition creation/deletion\n* Because listing large number of files in a directory is often slower than reading the list of files from the transaction log, you can use gneric WHERE clause instead of trying to optimize by loading single partitions explicitly\n  * Use spark.read.delta(\"/data\").where(\"partitionKey = 'value'\")\n  * as opposed to spark.read.format(\"parquet\").load(\"/data/partitionKey=value\")\n* The delta transactional log is the source of truth for the state of data\n  * Do not manually manipulate the data/log files as you can invalidate the data\n* Use the language, services, connectors, or database of your choice with Delta Lake and Delta Sharing. \n  * https://delta.io/integrations/"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bfa20dbb-3642-4b4e-bc35-6913a757bbbb"}}},{"cell_type":"markdown","source":["# Do you have the buy in for your Data Initiative?\n* Is the Business Use Case and value proposition clearly articulated?\n* Have businesss requirements been captured, prioritized and agreed upon? (Functional & Non-Functional)\n* Are stakeholders and champions vessted with a timeline and budget?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"37a6e57d-b808-484d-9490-7274796d27c5"}}},{"cell_type":"markdown","source":["# Developing your data product/service for production\n* Have businesss requirements been mapped to clear technical Requirements (Functional & Non-Functional)\n* Access to Data?\n  * Identify sensitive data sets\n  * Ascrtain user priveleges\n* Are folks available and trained to work on the big data initiative?\n  * Identify enablement and training needs\n* For a multi-tenancy scenario, have all access considerations been accounted for?\n  * Understanding the collaboration and IIsolation needs of data teams"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b4d3b9e8-2f0c-4c22-a668-93be9e712b08"}}},{"cell_type":"markdown","source":["# Data Migration Plays\n* Use Ventor tools to automate bulk of Migrating from other data systems such as Hadoop, Oracle, Netezza, Teradata\n  * Data storage\n  * Metadata storage\n  * Code migration around compatible libraries and APIs\n  * Data processing and transformations\n  * Security\n  * Orchestration of jobs and workflows\n* Fix workloads that cannot be automated\n* Establish performance benchmarks before and after\n* Tune the expensive workloads\n* Run workloads in parallel before switching off older systems"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e98edf7f-515e-4b9c-a58b-d683a5556c6a"}}},{"cell_type":"markdown","source":["* Convert a Parquet table to Delta\n  * CONVERT TO DELTA `<parquet table>`\n* Convert files to Delta format and create a table using that data\n  * CONVERT TO DELTA parquet.<`/data-path/`>\n  * CREATE TABLE `<delta table>` USING DELTA LOCATION <’/data-path/’>\n* Convert a non-Parquet format such as ORC to Parquet and then to Delta\n  * Read dataframe as orc and save as parquet\n  * CREATE TABLE `<parquet table>` USING PARQUET LOCATION <’/data-path/’>\n  * CONVERT TO DELTA `<parquet table>`\n* Generate a manifest file that can be read by other processing engines\n  * GENERATE symlink_format_manifest FOR TABLE `<delta table>`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9233cfc0-7150-4703-a4d8-ee5fadb7ed42"}}},{"cell_type":"markdown","source":["## Parquet to Delta\n* Conversion in place\n* Creates a Delta Lake transaction log that tracks all files in provided directory,  automatically infers the  schema\n* Collects statistics to improve query performance on the converted Delta table. \n* If table name is provided, the metastore is also updated \n* Run Additional OPTIMIZE/ZORDER for better performance on Delta\n* Caution\n  * Avoid changes to the data files during the conversion process. \n  * If multiple external tables share the same underlying Parquet directory, all of them should be converted\n  * Turning off stats collection using NO STATISTICS will hasten the conversion process\n  * If table has partitions, use the PARTITIONED BY clause on appropriate column/s"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4fa4cfba-646a-439e-81e8-10f0c8a71921"}}},{"cell_type":"code","source":["#simulate a parquet data path and parquet table\ncolumns = [\"State\",\"Name\", \"Age\"]\ndata = [(\"TX\",\"Jack\", 25), (\"NV\",\"Jane\",66), (\"CO\",\"Bill\",79),(\"CA\",\"Tom\",53), (\"WY\",\"Shawn\",45)]\nage_df = spark.sparkContext.parallelize(data).toDF(columns)\nage_location = DELTALAKE_PATH+'/demographic'\nage_df.write.format('parquet').save(age_location)\n\ns_sql = \"CREATE TABLE IF NOT EXISTS demographic USING parquet LOCATION '\" + age_location + \"'\"\nspark.sql(s_sql)\nspark.sql('DESCRIBE EXTENDED demographic')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"81f0d086-b555-4881-ac32-76e13a4a1de8"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df = spark.sql('DESCRIBE EXTENDED demographic')\ndf.filter(df.col_name.like('Provider')).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"92f007ee-5487-4993-89dc-ddf3e6332e39"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Convert a Parquet table to Delta\nspark.sql('CONVERT TO DELTA demographic')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f3f22778-549a-45ad-8052-68c489502e80"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df = spark.sql('DESCRIBE EXTENDED demographic')\ndf.filter(df.col_name.like('Provider')).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bded19f8-68af-48a5-b5c4-55f6807643e6"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Non-Parquet, Eg. ORC to Delta\n* first convert to parquet, then to delta"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5a931578-b0be-48b8-b9f3-be759e880570"}}},{"cell_type":"code","source":["age_location_orc = DELTALAKE_PATH+'/demographic_orc'\nage_df.write.format('orc').save(age_location_orc)\n\ns_sql = \"CREATE TABLE IF NOT EXISTS demographic_orc USING ORC LOCATION '\" + age_location_orc + \"'\"\nspark.sql(s_sql)\n\ndf_orc = spark.sql('DESCRIBE EXTENDED demographic_orc')\ndf_orc.filter(df_orc.col_name.like('Provider')).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b75745d3-b8c4-4bf0-876a-8dfeaab42a85"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["orc_df = spark.sql(\"SELECT * FROM demographic_orc\")\nage_location_orc_pq = DELTALAKE_PATH+'/demographic_orc_pq'\norc_df.write.format('parquet').save(age_location_orc_pq)\ns_sql = \"CREATE TABLE IF NOT EXISTS demographic_orc_pq USING parquet LOCATION '\" + age_location_orc_pq + \"'\"\nspark.sql(s_sql)\n\ndf_orc_pq = spark.sql('DESCRIBE EXTENDED demographic_orc_pq')\ndf_orc_pq.filter(df_orc_pq.col_name.like('Provider')).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2b70a5f6-03d2-4d66-ad2e-f47ec862a1f1"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["spark.sql('CONVERT TO DELTA demographic_orc_pq')\ndf_orc_pq_delta = spark.sql('DESCRIBE EXTENDED demographic_orc_pq')\ndf_orc_pq_delta.filter(df_orc_pq_delta.col_name.like('Provider')).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"24120f92-358d-41bb-a850-a1a31d648dd3"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Undo Conversion Operation\n* VACUUM delta.`<path-to-table>` RETAIN 0 HOURS\n* Delete the <path-to-table>/_delta_log directory."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ea68ce5b-271c-441a-8827-0c6b09e34ea8"}}},{"cell_type":"markdown","source":["# Capacity Planning\n* Understand Consumption patterns\n  * Number of total users & concurrent users\n  * Benchmark known and reepresentative queries\n* Understand Data Volumes & Processing needs \n  * per day/per month/per year\n  * Establish a yarly projection\n  * Extend to multi-year with buffer for growth of use cases \n* Benchmarking to ascertain Cluster type and sizing\n* How many environments ?\n  * Dev/Staging/Prod ?\n  * Planning for Disaster Recovery"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eee92f3c-09b0-4188-bde8-ee7403639538"}}},{"cell_type":"markdown","source":["# Data Democratization\n* Via Policy & Process\n* Identify needs for Delta Data Sharing"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"595defa6-0149-4571-8f6a-c3f394a04b6c"}}},{"cell_type":"markdown","source":["# Managing & Monitoring\n* Audit Logs\n* Cluster Logs\n* Spark Metrics\n* Sytem Metrics\n* Custom Logging"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1c2c3fc0-6c80-49ff-acfe-0e9255dd466a"}}},{"cell_type":"markdown","source":["# Establish COE\n* Responsible for\n  * Infrastructure, on-prem/cloud strategy\n  * Centralized Governance and Security\n  * Approving architecture blueprints\n  * Enablement/Training\n  * Reporting on usage and chargeback\n  * Automation"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"167fb4a8-419c-4915-8ad6-d0b00bfc6a21"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Ch-13-MappingYourDeltaJourney","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2,"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":1267259906084475,"dataframes":["_sqldf"]}},"language":"python","widgets":{},"notebookOrigID":844573653885267}},"nbformat":4,"nbformat_minor":0}
